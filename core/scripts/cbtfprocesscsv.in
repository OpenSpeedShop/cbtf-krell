#!/usr/bin/env python

# must be at beginning of file. support for python 2.6 and later
# where print statements syntax is a function. eg. print(). 
from __future__ import print_function
import os
import sys
import socket
import datetime

sys.path.insert(0,'@cbtfpythonmodules@')
import cbtfcsvtool


# Since we support hybrid, we need to aggrgegate per thread in a process.
# mpi programs typically only report mpi time in the main thread (0).
# OpenMPI (and possibly other mpi implementations) launch a helper thread
# that is internal mpi implementation data (eg nbody).  In these cases the
# main thread has the users mpi program metric data.  For mpi+openmpi, there
# this extra thread should not be confused with the openmp threads.
# The summary collector typicaly should ignore threads created
# during mpi startup.
#
#
# Main loop to compute min,max,avg metrics.
#
# Metrics computed per thread found. For hybrid mpi+openmp codes, these
# will typically be the openmp threads.  The summary collector ignores
# helper threads spawned during mpi startup.
#
# Since we use libmonitor thread nums in the collector, the thread id's
# will not match openmp_get_thread_num id's if any other threads are
# created before the openmp thread pool.
# In all cases, thread 0 is the master process thread.
#
def createReport():
    with open(reportfile, 'w') as fout:
        exe_name = csv_record.thread_data[0][0]['SUMMARY'][0]['executable']
        summary_header = "App_name,Hostname"
        summary_data = exe_name + ',' + socket.gethostname() 
        slurm_job_name = os.environ.get('SLURM_JOB_NAME')
        if slurm_job_name:
            summary_header += ",Slurm_job_name,Slurm_job_id"
            summary_data += ',' + slurm_job_name + ',' + \
                            os.environ['SLURM_JOB_ID']
        summary_header += ',Total_job_time'
        total_time = float(csv_record.calculate_max_time())
        summary_data += ',' + str(datetime.timedelta(seconds=total_time))
        fout.write(summary_header + '\n')
        fout.write(summary_data + '\n\n')
	# TODO: need to improve these headers regarding threads and ranks.
	# For an mpi job with no sub threads, thread 0 in N ranks.
	# For an mpi job with sub threads, thread n in N ranks.
	# For a sequential job with no sub threads, thread 0 in 1 process.
	# For a sequential job with sub threads, thread n in 1 process.

        for t in csv_record.thread_list:
            max_data = csv_record.thread_data[t][0]
            min_data = csv_record.thread_data[t][1]
            sum_data = csv_record.thread_data[t][2]
            count = csv_record.thread_data[t][3]
    
            def extract_data(section, key):
                 return [max_data[section][0][key], 
                         min_data[section][0][key], 
                         sum_data[section][0][key]/count]
    
            if csv_record.thread_cnt > 0:
                fout.write("Metrics for thread " + str(t) + " in " + str(count) + " ranks\n\n")
            else:
                fout.write("Metrics for " + str(count) + " ranks\n\n")
    

            for mx,mn,sm, in zip(max_data.iteritems(),
                                 min_data.iteritems(),
                                 sum_data.iteritems()):
                fout.write(mx[0] + '_metric_name,max,min,sum,avg\n')
     # Python dictionary hashing can create cases where dictionaries that
     # have matching keys can have differing indicies in the dictionary.
     # We simple use the keys found in max to lookup their values in the
     # min and sum dictionaries to keep the values aligned across dictionaries.
                for rmx,rmn,rsm, in zip(mx[1],mn[1],sm[1]):
                    for imx in rmx.iteritems():
                    # ignore these entries for min,max,avg.
                        if imx[0] in ('host','pid','rank','tid','posix_tid',
                                      'executable'):
                            continue
                        minval = rmn.get(imx[0])
                        maxval = rmx.get(imx[0])
                        sumval = rsm.get(imx[0])

                       # ignore values of None.  This will be the case when and
                       # incoming csv file does not have a key for one of the
                       # categories of data (eg. IO or MPI etc).
                        if minval == None or sumval == None or maxval == None:
                            continue

                        # do not print fields where there are no values
                        # greater than zero.
                        if minval == 0 and sumval == 0 and maxval == 0:
                            continue
                        fout.write(','.join([imx[0], str(imx[1]), str(minval), 
                                   str(sumval), str(sumval/count)]) + '\n')
                fout.write('\n')


#
# global object to hold process csv data
#
csv_record = cbtfcsvtool.CSV_Record(sys.argv[1])

# Name of the csv file to create
reportfile = csv_record.data_dirname + "-report.txt"

#
# main function to compute aggregated data report.
# This will create a text report file of any output
# printed to stdout.
#
createReport()
